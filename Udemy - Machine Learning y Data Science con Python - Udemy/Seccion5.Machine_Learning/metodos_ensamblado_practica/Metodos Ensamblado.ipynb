{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-07T20:57:58+01:00\n",
      "\n",
      "CPython 3.6.1\n",
      "IPython 5.3.0\n",
      "\n",
      "compiler   : GCC 4.8.2 20140120 (Red Hat 4.8.2-15)\n",
      "system     : Linux\n",
      "release    : 4.10.0-38-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(datos.data, columns=datos.feature_names)\n",
    "boston[\"objetivo\"] = datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>objetivo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  objetivo  \n",
       "0     15.3  396.90   4.98      24.0  \n",
       "1     17.8  396.90   9.14      21.6  \n",
       "2     17.8  392.83   4.03      34.7  \n",
       "3     18.7  394.63   2.94      33.4  \n",
       "4     18.7  396.90   5.33      36.2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INCISO**\n",
    "\n",
    "Hasta ahora hemos usado la magia de jupyter notebook `?` y `??` para ver cual es la documentacion de una clase o función. Jupyter lo unico que hace es leer el docstring de dichos objetos. El docstring de un objeto en python es el string delimitado por `\"\"\"` que se define justo después del nombre de dicho objeto, y tiene como objetivo el documentar el uso del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, si creamos la clase `ClaseTest` con un docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClaseTest():\n",
    "    \"\"\"Éste es el docstring\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podemos usar la magia de jupyter para que nos imprima el docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ClaseTest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente podemos imprimir directamente el atributo `__doc__` de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Éste es el docstring\n"
     ]
    }
   ],
   "source": [
    "print(ClaseTest.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a evaluar los algoritmos que conocemos hasta ahora y compararlos con los distintos algoritmos de ensamblado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def rmse_cv(estimador, X, y):\n",
    "    preds = estimador.predict(X)\n",
    "    return rmse(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultados = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arbol': 5.9465687016858997,\n",
       " 'elasticnet': 5.2610502191314854,\n",
       " 'lasso': 5.4649203446998555,\n",
       " 'ridge': 5.0996452049612477}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "estimador_arbol = DecisionTreeRegressor()\n",
    "\n",
    "error_cv = cross_val_score(estimador_arbol, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"arbol\"] = error_cv\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge\n",
    "\n",
    "estimador_elnet = ElasticNet()\n",
    "\n",
    "resultados[\"elasticnet\"] = cross_val_score(estimador_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "\n",
    "estimador_lasso = Lasso()\n",
    "estimador_ridge = Ridge()\n",
    "\n",
    "\n",
    "resultados[\"lasso\"] = cross_val_score(estimador_lasso, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"ridge\"] = cross_val_score(estimador_ridge, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de Bagging (Bootstrap aggregating) funcionan entrenando varios estimadores base y cambiando los datos de entrenamiento para cada uno. En sklearn los algoritmos de ensamblado de modelos se encuentran en el submódulo `sklearn.ensemble`. En cuanto a Bagging, sklearn tiene una versión para problemas de regresión (`BaggingRegressor`) y otra para problemas de clasificación (`BaggingClassifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Bagging regressor.\n",
      "\n",
      "    A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "    regressors each on random subsets of the original dataset and then\n",
      "    aggregate their individual predictions (either by voting or by averaging)\n",
      "    to form a final prediction. Such a meta-estimator can typically be used as\n",
      "    a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "    tree), by introducing randomization into its construction procedure and\n",
      "    then making an ensemble out of it.\n",
      "\n",
      "    This algorithm encompasses several works from the literature. When random\n",
      "    subsets of the dataset are drawn as random subsets of the samples, then\n",
      "    this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "    replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "    of the dataset are drawn as random subsets of the features, then the method\n",
      "    is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "    on subsets of both samples and features, then the method is known as\n",
      "    Random Patches [4]_.\n",
      "\n",
      "    Read more in the :ref:`User Guide <bagging>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object or None, optional (default=None)\n",
      "        The base estimator to fit on random subsets of the dataset.\n",
      "        If None, then the base estimator is a decision tree.\n",
      "\n",
      "    n_estimators : int, optional (default=10)\n",
      "        The number of base estimators in the ensemble.\n",
      "\n",
      "    max_samples : int or float, optional (default=1.0)\n",
      "        The number of samples to draw from X to train each base estimator.\n",
      "            - If int, then draw `max_samples` samples.\n",
      "            - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "\n",
      "    max_features : int or float, optional (default=1.0)\n",
      "        The number of features to draw from X to train each base estimator.\n",
      "            - If int, then draw `max_features` features.\n",
      "            - If float, then draw `max_features * X.shape[1]` features.\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether samples are drawn with replacement.\n",
      "\n",
      "    bootstrap_features : boolean, optional (default=False)\n",
      "        Whether features are drawn with replacement.\n",
      "\n",
      "    oob_score : bool\n",
      "        Whether to use out-of-bag samples to estimate\n",
      "        the generalization error.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to True, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit\n",
      "        a whole new ensemble.\n",
      "\n",
      "    n_jobs : int, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the building process.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of estimators\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimators_samples_ : list of arrays\n",
      "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "        estimator. Each subset is defined by a boolean mask.\n",
      "\n",
      "    estimators_features_ : list of arrays\n",
      "        The subset of drawn features for each base estimator.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    oob_prediction_ : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_prediction_` might contain NaN.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "\n",
      "    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "           1996.\n",
      "\n",
      "    .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "           1998.\n",
      "\n",
      "    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(BaggingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4643651472573547"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_10 = BaggingRegressor(n_estimators=10)\n",
    "error_cv = cross_val_score(estimador_bagging_10, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_arbol_10\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentar el número de estimadores base es una forma limitada pero sencilla de mejorar el funcionamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2092328950958855"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_100 = BaggingRegressor(n_estimators=100)\n",
    "error_cv = cross_val_score(estimador_bagging_100, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_arbol_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BaggingRegressor` utiliza árboles de decisión como estimador base por defecto, sin embargo podemos utilizar uno distinto mediante el parámetro `base_estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2666637956257238"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ElasticNet())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_elnet\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En su momento vimos que existe un tipo de arbol de decision completamente aleatorio (Extremely Randomized Trees) que deciden la particion en cada nodo al azar. Vemos que al agrupar muchos de estos estimadores que son débiles (aun que mejores que tirar una moneda al azar, ya que un árbol de decision aleatorio aun asi aprende a separar los elementos), la varianza general se reduce ya que la que aporta un arbol se complementa con la del de al lado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9164564364820924"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import ExtraTreeRegressor\n",
    "\n",
    "estimador_bagging_elnet = BaggingRegressor(n_estimators=100, base_estimator=ExtraTreeRegressor())\n",
    "error_cv = cross_val_score(estimador_bagging_elnet, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"bagging_extra_arbol\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los algoritmos de boosting intentan mejorar los estimadores base asignando pesos en funcion de su funcionamiento individual. El algoritmo clásico de boosting es `AdaBoost`, que se encuentra en sklearn como `AdaBoostRegressor` y `AdaBoostClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An AdaBoost regressor.\n",
      "\n",
      "    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "    regressor on the original dataset and then fits additional copies of the\n",
      "    regressor on the same dataset but where the weights of instances are\n",
      "    adjusted according to the error of the current prediction. As such,\n",
      "    subsequent regressors focus more on difficult cases.\n",
      "\n",
      "    This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "\n",
      "    Read more in the :ref:`User Guide <adaboost>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    base_estimator : object, optional (default=DecisionTreeRegressor)\n",
      "        The base estimator from which the boosted ensemble is built.\n",
      "        Support for sample weighting is required.\n",
      "\n",
      "    n_estimators : integer, optional (default=50)\n",
      "        The maximum number of estimators at which boosting is terminated.\n",
      "        In case of perfect fit, the learning procedure is stopped early.\n",
      "\n",
      "    learning_rate : float, optional (default=1.)\n",
      "        Learning rate shrinks the contribution of each regressor by\n",
      "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
      "        ``n_estimators``.\n",
      "\n",
      "    loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
      "        The loss function to use when updating the weights after each\n",
      "        boosting iteration.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of classifiers\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    estimator_weights_ : array of floats\n",
      "        Weights for each estimator in the boosted ensemble.\n",
      "\n",
      "    estimator_errors_ : array of floats\n",
      "        Regression error for each estimator in the boosted ensemble.\n",
      "\n",
      "    feature_importances_ : array of shape = [n_features]\n",
      "        The feature importances if supported by the ``base_estimator``.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "           on-Line Learning and an Application to Boosting\", 1995.\n",
      "\n",
      "    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(AdaBoostRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5006246116617605"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_adaboost = AdaBoostRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_adaboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"adaboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GBRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro algoritmo de Boosting es Gradient Boosting que a cada iteración usa el algoritmo de Descenso de Gradiente (que veremos en el futuro) para a cada iteración , entrenar un estimador nuevo que minimiza la función de error (*loss function*) del modelo.\n",
    "\n",
    "Scikit-learn implementa el algoritmo de (Gradient Boosted Regression Trees), que usa árboles de decisión como estimadores base, en [GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) y [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "Gradient Boosting puede usar cualquier funcion de error siempre que sea diferenciable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting for regression.\n",
      "\n",
      "    GB builds an additive model in a forward stage-wise fashion;\n",
      "    it allows for the optimization of arbitrary differentiable loss functions.\n",
      "    In each stage a regression tree is fit on the negative gradient of the\n",
      "    given loss function.\n",
      "\n",
      "    Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n",
      "        loss function to be optimized. 'ls' refers to least squares\n",
      "        regression. 'lad' (least absolute deviation) is a highly robust\n",
      "        loss function solely based on order information of the input\n",
      "        variables. 'huber' is a combination of the two. 'quantile'\n",
      "        allows quantile regression (use `alpha` to specify the quantile).\n",
      "\n",
      "    learning_rate : float, optional (default=0.1)\n",
      "        learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "        There is a trade-off between learning_rate and n_estimators.\n",
      "\n",
      "    n_estimators : int (default=100)\n",
      "        The number of boosting stages to perform. Gradient boosting\n",
      "        is fairly robust to over-fitting so a large number usually\n",
      "        results in better performance.\n",
      "\n",
      "    max_depth : integer, optional (default=3)\n",
      "        maximum depth of the individual regression estimators. The maximum\n",
      "        depth limits the number of nodes in the tree. Tune this parameter\n",
      "        for best performance; the best value depends on the interaction\n",
      "        of the input variables.\n",
      "\n",
      "    criterion : string, optional (default=\"friedman_mse\")\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"friedman_mse\" for the mean squared error with improvement\n",
      "        score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "        the mean absolute error. The default value of \"friedman_mse\" is\n",
      "        generally the best as it can provide a better approximation in\n",
      "        some cases.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a percentage and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node:\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a percentage and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    subsample : float, optional (default=1.0)\n",
      "        The fraction of samples to be used for fitting the individual base\n",
      "        learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "        Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "        Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=None)\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a percentage and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Choosing `max_features < n_features` leads to a reduction of variance\n",
      "        and an increase in bias.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_split : float,\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "           Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    alpha : float (default=0.9)\n",
      "        The alpha-quantile of the huber loss function and the quantile\n",
      "        loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "\n",
      "    init : BaseEstimator, None, optional (default=None)\n",
      "        An estimator object that is used to compute the initial\n",
      "        predictions. ``init`` has to provide ``fit`` and ``predict``.\n",
      "        If None it uses ``loss.init_estimator``.\n",
      "\n",
      "    verbose : int, default: 0\n",
      "        Enable verbose output. If 1 then it prints progress and performance\n",
      "        once in a while (the more trees the lower the frequency). If greater\n",
      "        than 1 then it prints progress and performance for every tree.\n",
      "\n",
      "    warm_start : bool, default: False\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just erase the\n",
      "        previous solution.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    presort : bool or 'auto', optional (default='auto')\n",
      "        Whether to presort the data to speed up the finding of best splits in\n",
      "        fitting. Auto mode by default will use presorting on dense data and\n",
      "        default to normal sorting on sparse data. Setting presort to true on\n",
      "        sparse data will raise an error.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           optional parameter *presort*.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    feature_importances_ : array, shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    oob_improvement_ : array, shape = [n_estimators]\n",
      "        The improvement in loss (= deviance) on the out-of-bag samples\n",
      "        relative to the previous iteration.\n",
      "        ``oob_improvement_[0]`` is the improvement in\n",
      "        loss of the first stage over the ``init`` estimator.\n",
      "\n",
      "    train_score_ : array, shape = [n_estimators]\n",
      "        The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "        model at iteration ``i`` on the in-bag sample.\n",
      "        If ``subsample == 1`` this is the deviance on the training data.\n",
      "\n",
      "    loss_ : LossFunction\n",
      "        The concrete ``LossFunction`` object.\n",
      "\n",
      "    init : BaseEstimator\n",
      "        The estimator that provides the initial predictions.\n",
      "        Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "\n",
      "    estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data and\n",
      "    ``max_features=n_features``, if the improvement of the criterion is\n",
      "    identical for several splits enumerated during the search of the best\n",
      "    split. To obtain a deterministic behaviour during fitting,\n",
      "    ``random_state`` has to be fixed.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor, RandomForestRegressor\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "\n",
      "    J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "\n",
      "    T. Hastie, R. Tibshirani and J. Friedman.\n",
      "    Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(GradientBoostingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9206439598661236"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_gradientboost = GradientBoostingRegressor(n_estimators=100, loss='ls')\n",
    "\n",
    "error_cv = cross_val_score(estimador_gradientboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"gradientboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cualquier estimador basado en árboles, `GradientBoostRegressor` nos permite ver la importancia de las variables en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7b9ec6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimador_gradientboost.fit(boston[datos.feature_names], boston.objetivo)\n",
    "\n",
    "importancia_variables = estimador_gradientboost.feature_importances_\n",
    "importancia_variables = 100.0 * (importancia_variables / importancia_variables.max())\n",
    "sorted_idx = np.argsort(importancia_variables)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.barh(pos, importancia_variables[sorted_idx], align='center')\n",
    "plt.yticks(pos, datos.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bosques Aleatorios (Random Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de Bosques Aleatorios funciona mediante la creación de árboles de decision entrenados en un subgrupo aleatorio de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest regressor.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of classifying\n",
      "    decision trees on various sub-samples of the dataset and use averaging\n",
      "    to improve the predictive accuracy and control over-fitting.\n",
      "    The sub-sample size is always the same as the original\n",
      "    input sample size but the samples are drawn with replacement if\n",
      "    `bootstrap=True` (default).\n",
      "\n",
      "    Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : integer, optional (default=10)\n",
      "        The number of trees in the forest.\n",
      "\n",
      "    criterion : string, optional (default=\"mse\")\n",
      "        The function to measure the quality of a split. Supported criteria\n",
      "        are \"mse\" for the mean squared error, which is equal to variance\n",
      "        reduction as feature selection criterion, and \"mae\" for the mean\n",
      "        absolute error.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "           Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "    max_features : int, float, string or None, optional (default=\"auto\")\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a percentage and\n",
      "          `int(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=n_features`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_depth : integer or None, optional (default=None)\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int, float, optional (default=2)\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a percentage and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_samples_leaf : int, float, optional (default=1)\n",
      "        The minimum number of samples required to be at a leaf node:\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a percentage and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for percentages.\n",
      "\n",
      "    min_weight_fraction_leaf : float, optional (default=0.)\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_leaf_nodes : int or None, optional (default=None)\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_split : float,\n",
      "        Threshold for early stopping in tree growth. A node will split\n",
      "        if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "        .. deprecated:: 0.19\n",
      "           ``min_impurity_split`` has been deprecated in favor of\n",
      "           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.\n",
      "           Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "    min_impurity_decrease : float, optional (default=0.)\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    bootstrap : boolean, optional (default=True)\n",
      "        Whether bootstrap samples are used when building trees.\n",
      "\n",
      "    oob_score : bool, optional (default=False)\n",
      "        whether to use out-of-bag samples to estimate\n",
      "        the R^2 on unseen data.\n",
      "\n",
      "    n_jobs : integer, optional (default=1)\n",
      "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
      "        If -1, then the number of jobs is set to the number of cores.\n",
      "\n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the tree building process.\n",
      "\n",
      "    warm_start : bool, optional (default=False)\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "        new forest.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    estimators_ : list of DecisionTreeRegressor\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    feature_importances_ : array of shape = [n_features]\n",
      "        The feature importances (the higher, the more important the feature).\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "\n",
      "    oob_prediction_ : array of shape = [n_samples]\n",
      "        Prediction computed with out-of-bag estimate on the training set.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.ensemble import RandomForestRegressor\n",
      "    >>> from sklearn.datasets import make_regression\n",
      "    >>>\n",
      "    >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "    ...                        random_state=0, shuffle=False)\n",
      "    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      "    >>> regr.fit(X, y)\n",
      "    RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      "               max_features='auto', max_leaf_nodes=None,\n",
      "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "               min_samples_leaf=1, min_samples_split=2,\n",
      "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "               oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "    >>> print(regr.feature_importances_)\n",
      "    [ 0.17339552  0.81594114  0.          0.01066333]\n",
      "    >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "    [-2.50699856]\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data,\n",
      "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "    of the criterion is identical for several splits enumerated during the\n",
      "    search of the best split. To obtain a deterministic behaviour during\n",
      "    fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    See also\n",
      "    --------\n",
      "    DecisionTreeRegressor, ExtraTreesRegressor\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(RandomForestRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de scikit-learn de RandomForest hace que cada árbol se entrene en base a un dataset del mismo tamaño que el original (con reemplazo si se usa la opción `bootstrap=True`).\n",
    "\n",
    "En cuanto al criterio para evaluar la calidad de la separación de un node de cada árbol base, para la implementación de Regresion, `RandomForestRegressor` usa el error medio cuadrático `mse` por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1939664948606872"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_randomforest = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_randomforest, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"randomforest_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboost_100': 4.5006246116617605,\n",
       " 'arbol': 5.9465687016858997,\n",
       " 'bagging_arbol_10': 4.4643651472573547,\n",
       " 'bagging_arbol_100': 4.2092328950958855,\n",
       " 'bagging_elnet': 5.2666637956257238,\n",
       " 'bagging_extra_arbol': 3.9164564364820924,\n",
       " 'elasticnet': 5.2610502191314854,\n",
       " 'gradientboost_100': 3.9206439598661236,\n",
       " 'lasso': 5.4649203446998555,\n",
       " 'randomforest_100': 4.1939664948606872,\n",
       " 'ridge': 5.0996452049612477}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) es un algoritmo de boosting relativamente nuevo que tiene bastante acogida. Es una implementación de Gradient Boosted Trees pero enfocado a datasets grandes.\n",
    "\n",
    "Al ser muy nuevo (el proyecto se creó en 2014 y el paper se publicó en 2016, [éste es el paper](https://arxiv.org/abs/1603.02754)) no está implementado en scikit-learn, sin embargo existe en el paquete [xgboost](http://xgboost.readthedocs.io/en/latest/python/python_intro.html), que proporciona estimadores en base a dicho algoritmo que son compatibles con sklearn.\n",
    "\n",
    "Podemos instalar `xgboost` de conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .........^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manuel/anaconda3/envs/data/bin/conda\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/cli/main.py\", line 182, in main\n",
      "    return conda_exception_handler(_main, *args)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/exceptions.py\", line 640, in conda_exception_handler\n",
      "    return_value = func(*args, **kwargs)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/cli/main.py\", line 140, in _main\n",
      "    exit_code = args.func(args, p)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/cli/main_install.py\", line 80, in execute\n",
      "    install(args, parser, 'install')\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/cli/install.py\", line 231, in install\n",
      "    unknown=index_args['unknown'], prefix=prefix)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/core/index.py\", line 101, in get_index\n",
      "    index = fetch_index(channel_priority_map, use_cache=use_cache)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/core/index.py\", line 120, in fetch_index\n",
      "    repodatas = collect_all_repodata(use_cache, tasks)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/core/repodata.py\", line 75, in collect_all_repodata\n",
      "    repodatas = _collect_repodatas_serial(use_cache, tasks)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/core/repodata.py\", line 485, in _collect_repodatas_serial\n",
      "    for url, schan, pri in tasks]\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/core/repodata.py\", line 485, in <listcomp>\n",
      "    for url, schan, pri in tasks]\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/core/repodata.py\", line 115, in func\n",
      "    res = f(*args, **kwargs)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/site-packages/conda/core/repodata.py\", line 474, in fetch_repodata\n",
      "    json.dump(repodata, fo, indent=2, sort_keys=True, cls=EntityEncoder)\n",
      "  File \"/home/manuel/anaconda3/lib/python3.6/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/anaconda3/envs/data/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "        Parameters\n",
      "    ----------\n",
      "    max_depth : int\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : float\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    n_estimators : int\n",
      "        Number of boosted trees to fit.\n",
      "    silent : boolean\n",
      "        Whether to print messages while running boosting.\n",
      "    objective : string or callable\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    nthread : int\n",
      "        Number of parallel threads used to run xgboost.\n",
      "    gamma : float\n",
      "        Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "    min_child_weight : int\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : int\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : float\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : float\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : float\n",
      "        Subsample ratio of columns for each split, in each level.\n",
      "    reg_alpha : float (xgb's alpha)\n",
      "        L1 regularization term on weights\n",
      "    reg_lambda : float (xgb's lambda)\n",
      "        L2 regularization term on weights\n",
      "    scale_pos_weight : float\n",
      "        Balancing of positive and negative weights.\n",
      "\n",
      "    base_score:\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    seed : int\n",
      "        Random number seed.\n",
      "    missing : float, optional\n",
      "        Value in the data which needs to be present as a missing value. If\n",
      "        None, defaults to np.nan.\n",
      "\n",
      "    Note\n",
      "    ----\n",
      "    A custom objective function can be provided for the ``objective``\n",
      "    parameter. In this case, it should have the signature\n",
      "    ``objective(y_true, y_pred) -> grad, hess``:\n",
      "\n",
      "    y_true: array_like of shape [n_samples]\n",
      "        The target values\n",
      "    y_pred: array_like of shape [n_samples]\n",
      "        The predicted values\n",
      "\n",
      "    grad: array_like of shape [n_samples]\n",
      "        The value of the gradient for each sample point.\n",
      "    hess: array_like of shape [n_samples]\n",
      "        The value of the second derivative for each sample point\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(XGBRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9976500479974781"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_xgboost = XGBRegressor(n_estimators=100)\n",
    "\n",
    "error_cv = cross_val_score(estimador_xgboost, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"xgboost_100\"] = error_cv\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import plot_importance, to_graphviz\n",
    "estimador_xgboost.fit(boston[datos.feature_names], boston.objetivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos un modelo entrenado, podemos ver la importancia de las variables en partir los árboles mediante el método `plot_importance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe7e5978710>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEWCAYAAAAgpUMxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOeci4QXIOedcJP4/A6Ye6FX8+ggAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7b9ef3978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(estimador_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `to_graphviz` imprime un árbol en concreto (pasandole el parámetro `rankdir='LR'` lo imprime en horizontal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"821pt\" height=\"555pt\"\n",
       " viewBox=\"0.00 0.00 821.27 554.84\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 550.839)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-550.839 817.265,-550.839 817.265,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54.5952\" cy=\"-278.694\" rx=\"54.6905\" ry=\"54.6905\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.5952\" y=\"-274.994\" font-family=\"Times,serif\" font-size=\"14.00\">RM&lt;6.5455</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"281.784\" cy=\"-336.694\" rx=\"67.6881\" ry=\"67.6881\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.784\" y=\"-332.994\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT&lt;19.645</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M107.541,-292.074C136.851,-299.623 173.978,-309.185 206.337,-317.52\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"205.655,-320.958 216.212,-320.063 207.401,-314.179 205.655,-320.958\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.69\" y=\"-317.494\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"281.784\" cy=\"-195.694\" rx=\"55.4913\" ry=\"55.4913\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.784\" y=\"-191.994\" font-family=\"Times,serif\" font-size=\"14.00\">NOX&lt;0.659</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M106.132,-260.068C139.795,-247.66 184.22,-231.286 219.84,-218.157\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"221.465,-221.289 229.637,-214.546 219.044,-214.721 221.465,-221.289\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.69\" y=\"-256.494\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"527.822\" cy=\"-487.694\" rx=\"59.2899\" ry=\"59.2899\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.822\" y=\"-483.994\" font-family=\"Times,serif\" font-size=\"14.00\">DIS&lt;1.37275</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M339.692,-371.907C378.319,-395.807 429.258,-427.326 468.235,-451.443\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"466.517,-454.496 476.863,-456.782 470.201,-448.544 466.517,-454.496\"/>\n",
       "<text text-anchor=\"middle\" x=\"401.878\" y=\"-433.494\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"527.822\" cy=\"-336.694\" rx=\"73.387\" ry=\"73.387\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.822\" y=\"-332.994\" font-family=\"Times,serif\" font-size=\"14.00\">PTRATIO&lt;19.65</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M349.469,-336.694C378.63,-336.694 413.142,-336.694 443.935,-336.694\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"444.222,-340.194 454.222,-336.694 444.222,-333.194 444.222,-340.194\"/>\n",
       "<text text-anchor=\"middle\" x=\"401.878\" y=\"-340.494\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node10\" class=\"node\"><title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"527.822\" cy=\"-195.694\" rx=\"50.0912\" ry=\"50.0912\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.822\" y=\"-191.994\" font-family=\"Times,serif\" font-size=\"14.00\">RM&lt;7.437</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;5 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>2&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M337.247,-195.694C375.886,-195.694 427.722,-195.694 467.424,-195.694\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"467.5,-199.194 477.5,-195.694 467.5,-192.194 467.5,-199.194\"/>\n",
       "<text text-anchor=\"middle\" x=\"401.878\" y=\"-199.494\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node11\" class=\"node\"><title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"527.822\" cy=\"-63.6943\" rx=\"63.8893\" ry=\"63.8893\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.822\" y=\"-59.9943\" font-family=\"Times,serif\" font-size=\"14.00\">LSTAT&lt;24.53</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M330.707,-169.773C368.339,-149.418 421.105,-120.877 462.426,-98.5263\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"464.196,-101.548 471.326,-93.712 460.865,-95.3912 464.196,-101.548\"/>\n",
       "<text text-anchor=\"middle\" x=\"401.878\" y=\"-153.494\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"804.265,-510.694 715.265,-510.694 715.265,-474.694 804.265,-474.694 804.265,-510.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-488.994\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=1.62421</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M587.144,-488.963C623.352,-489.75 669.555,-490.755 704.898,-491.523\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"704.953,-495.025 715.027,-491.743 705.105,-488.027 704.953,-495.025\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-494.494\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node7\" class=\"node\"><title>8</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"807.765,-456.694 711.765,-456.694 711.765,-420.694 807.765,-420.694 807.765,-456.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-434.994\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.602076</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;8 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>3&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M585.957,-475.519C621.185,-468.012 666.209,-458.418 701.468,-450.904\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"702.554,-454.251 711.605,-448.744 701.095,-447.405 702.554,-454.251\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-470.494\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node8\" class=\"node\"><title>9</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"807.765,-383.694 711.765,-383.694 711.765,-347.694 807.765,-347.694 807.765,-383.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-361.994\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.538199</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M600.946,-345.791C633.369,-349.88 671.098,-354.638 701.534,-358.476\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"701.199,-361.962 711.558,-359.741 702.075,-355.017 701.199,-361.962\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-359.494\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\"><title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"807.765,-329.694 711.765,-329.694 711.765,-293.694 807.765,-293.694 807.765,-329.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-307.994\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.224159</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>4&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M600.946,-328.853C633.369,-325.328 671.098,-321.226 701.534,-317.917\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"701.995,-321.387 711.558,-316.827 701.239,-314.428 701.995,-321.387\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-329.494\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"804.265,-255.694 715.265,-255.694 715.265,-219.694 804.265,-219.694 804.265,-255.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-233.994\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=1.00346</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>5&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M577.306,-204.55C614.681,-211.377 666.338,-220.812 704.982,-227.871\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"704.544,-231.348 715.01,-229.702 705.802,-224.462 704.544,-231.348\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-228.494\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"804.265,-201.694 715.265,-201.694 715.265,-165.694 804.265,-165.694 804.265,-201.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-179.994\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=1.51665</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>5&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M577.868,-193.135C615.122,-191.191 666.324,-188.519 704.748,-186.513\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"705.421,-189.983 715.225,-185.967 705.056,-182.993 705.421,-189.983\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-195.494\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"807.765,-108.694 711.765,-108.694 711.765,-72.6943 807.765,-72.6943 807.765,-108.694\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-86.9943\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=0.383431</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M591.351,-71.0366C625.7,-75.0698 667.997,-80.0363 701.482,-83.9681\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"701.232,-87.4627 711.572,-85.1529 702.048,-80.5105 701.232,-87.4627\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-86.4943\" font-family=\"Times,serif\" font-size=\"14.00\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"813.265,-54.6943 706.265,-54.6943 706.265,-18.6943 813.265,-18.6943 813.265,-54.6943\"/>\n",
       "<text text-anchor=\"middle\" x=\"759.765\" y=\"-32.9943\" font-family=\"Times,serif\" font-size=\"14.00\">leaf=&#45;0.0193481</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>6&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M591.351,-56.3521C623.752,-52.5476 663.224,-47.9128 695.702,-44.0992\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"696.46,-47.5344 705.983,-42.892 695.643,-40.5822 696.46,-47.5344\"/>\n",
       "<text text-anchor=\"middle\" x=\"653.765\" y=\"-56.4943\" font-family=\"Times,serif\" font-size=\"14.00\">no</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fe7e42c3ba8>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_graphviz(estimador_xgboost, num_trees=11, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, el output de un árbol no está en la misma escala que las predicciones (la variable objetivo tiene el rango 5-50), esto es así por que en el algoritmo XGBoost cada árbol se basa en el output del árbol anterior, intentando corregir el error producido por el mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de stacking simplemente usa el output (generalmente en terminos de probabilidades para casos de clasificacion o de las predicciones en casos de regresión) de múltiples modelos como input para un nuevo *metamodelo*.\n",
    "\n",
    "scikit learn no tiene un estimador de stacking por defecto, sin embargo, podemos usar el  estimador de stacking (`StackingRegressor`) de [mlxtend](https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/), una librería que amplia las funcionalidades de `sklearn`\n",
    "\n",
    "Podemos instalar mlxtend asi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c conda-forge mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.regressor import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Stacking regressor for scikit-learn estimators for regression.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    regressors : array-like, shape = [n_regressors]\n",
      "        A list of regressors.\n",
      "        Invoking the `fit` method on the `StackingRegressor` will fit clones\n",
      "        of those original regressors that will\n",
      "        be stored in the class attribute\n",
      "        `self.regr_`.\n",
      "    meta_regressor : object\n",
      "        The meta-regressor to be fitted on the ensemble of\n",
      "        regressors\n",
      "    verbose : int, optional (default=0)\n",
      "        Controls the verbosity of the building process.\n",
      "        - `verbose=0` (default): Prints nothing\n",
      "        - `verbose=1`: Prints the number & name of the regressor being fitted\n",
      "        - `verbose=2`: Prints info about the parameters of the\n",
      "                       regressor being fitted\n",
      "        - `verbose>2`: Changes `verbose` param of the underlying regressor to\n",
      "           self.verbose - 2\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    regr_ : list, shape=[n_regressors]\n",
      "        Fitted regressors (clones of the original regressors)\n",
      "    meta_regr_ : estimator\n",
      "        Fitted meta-regressor (clone of the original meta-estimator)\n",
      "    coef_ : array-like, shape = [n_features]\n",
      "        Model coefficients of the fitted meta-estimator\n",
      "    intercept_ : float\n",
      "        Intercept of the fitted meta-estimator\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(StackingRegressor.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, podemos usar los estimadores ensamblados que hemos creado en este notebook para crear un nuevo estimador. Dicho estimador no tiene garantizado un funcionamiento mejor que el mejor de los estimadores que usa como input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0863402204208503"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimador_stacking = StackingRegressor(\n",
    "    regressors=[\n",
    "        BaggingRegressor(n_estimators=100),\n",
    "        AdaBoostRegressor(n_estimators=100),\n",
    "        GradientBoostingRegressor(n_estimators=100),\n",
    "        RandomForestRegressor(n_estimators=100)\n",
    "    ], \n",
    "    meta_regressor=XGBRegressor(n_estimators=100))\n",
    "\n",
    "\n",
    "error_cv = cross_val_score(estimador_stacking, X=boston[datos.feature_names], y=boston[\"objetivo\"], \n",
    "                scoring=rmse_cv, cv=10).mean()\n",
    "\n",
    "resultados[\"stacking\"] = error_cv\n",
    "\n",
    "error_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adaboost_100': 4.5006246116617605,\n",
       " 'arbol': 5.9465687016858997,\n",
       " 'bagging_arbol_10': 4.4643651472573547,\n",
       " 'bagging_arbol_100': 4.2092328950958855,\n",
       " 'bagging_elnet': 5.2666637956257238,\n",
       " 'bagging_extra_arbol': 3.9164564364820924,\n",
       " 'elasticnet': 5.2610502191314854,\n",
       " 'gradientboost_100': 3.9206439598661236,\n",
       " 'lasso': 5.4649203446998555,\n",
       " 'randomforest_100': 4.1939664948606872,\n",
       " 'ridge': 5.0996452049612477,\n",
       " 'stacking': 4.0863402204208503,\n",
       " 'xgboost_100': 3.9976500479974781}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
